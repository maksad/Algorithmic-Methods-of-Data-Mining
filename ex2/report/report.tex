\documentclass[11pt,a4paper,english]{article}
  \usepackage[latin1]{inputenc}
  \usepackage{amsmath,amsfonts,amssymb}
  \usepackage{enumitem}
  \usepackage{fullpage}
  \usepackage{graphicx}
  \usepackage{tabto}
  \usepackage{etoolbox}
  \usepackage{hyperref}
  \usepackage{minted}
  \usepackage{parskip}
  \usepackage[font=small,labelfont=bf]{caption}
  \renewcommand{\labelenumii}{\theenumii}
  \renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}

  \title{Algorithmic Methods of Data Mining - Assignment 2}
  \author{Maksad Donayorov}

  \begin{document}
    \maketitle
    \definecolor{bg}{rgb}{0.95,0.95,0.95}

    \begin{enumerate}
      \item \textbf{Problem:} \textit{Bag of words:}
        \begin{enumerate}
          \item \textbf{Question:} \textit{Jaccard coefficient to bags is meaningful and well-motivated:}\\
            Jaccard coefficient is used for comparing the similarity and diversity of sets. As we are using a finite set to represent "bag of words" the extention of Jaccard index to bag is meaningful and well-motivated.

            For example: assume that we have two documents which are not similar and represented by sets: $A = \{(x, 2), (y, 3)\}$ and $B = \{(x, 1),(z, 3)\}$. The Jaccard coefficient for these documents will be:
            \begin{align*}
              J(A, B) = \frac{\|A \cap B\|}{\|A \cup B\|} =
              \frac{ \| \{x,x,y,y,y\} \cap \{x,z,z,z\} \| }{ \| \{x,x,y,y,y\} \cup \{x,z,z,z\} \| } & = \\
              \frac{ \| \{x\} \| }{ \| \{ x,x,y,y,y,z,z,z \} \| } & = \frac{1}{8} = 0.125
            \end{align*}
            As we can see the Jaccard index does prove that these two documents are not similar.
            On the contrary, if we take two exactly the same bags: $A = \{(x, 2), (y, 1)\}$ and $B = \{(x, 2),(y, 1)\}$. The the Jaccard index will be:
            \begin{align*}
              J(A, B) = \frac{\|A \cap B\|}{\|A \cup B\|} =
              \frac{ \| \{x,x,y\} \cap \{x,x,y\} \| }{ \| \{x,x,y\} \cup \{x,x,y\} \| } & = \\
              \frac{ \| \{x,x,y\} \| }{ \| \{ x,x,y \} \| } & = \frac{1}{1} = 1
            \end{align*}
            And again we can prove by Jaccard index that these two documents are exactly the same. Consequently, we can say that the extension of the Jaccard coefficient to bags, is
            meaningful and well-motivated.

          \item \textbf{Question:} \textit{Design a family of hash functions $\mathcal{F}$:}\\
            As mentioned in the part $1.1$ the Jaccard index measures how similar two sets are. Therefore, we can use \textbf{min-hash} locality-sensitive hashing scheme for Jaccard coefficient to bags. The algorithm is pretty straight forward:
            \begin{enumerate}
              \item for sets $A$ and $B$ that represent two documents
              \item calculated set $C$, such that $C = A \cup B $
              \item shuffle set $C$ and sample from it minimum element in permutation
              \item repeat this in a cycle
            \end{enumerate}

            Now coming back to the prove that min-hash function does satisfies this $Pr[f(A)= f(B)]=J(A,B)$ equality. As always, it is the best to have a good intuition and for that we can look at an example. Let's assume we have two bags:
            \begin{align*}
              A = \{ (x,2), (y,2), (z,1) \} \\
              B = \{ (x,2), (z,1) \} \\
            \end{align*}
            Let's now take the union of $A$ and $B$.
            \begin{align*}
              C = A \cup B = \{ (x,2), (y,2), (z,1) \} = \{ x_0, x_1, y_2, y_3, z_4 \} \\
            \end{align*}
            This union shows us that all the elements that $A$ and $B$ have and we can use this set in our "hash table". But before that let's think of some hash function. Based on the book a good hash function is the one that has a modulo of a prime number that is bigger than the number of elements in the union set $C$. Since we have 5 elements in $C$ we can choose $7$ for our modulo and create some hash functions:
            \begin{align*}
               (i + 1) & \ mod \ 7 \\
              2(i + 1) & \ mod \ 7 \\
              (i + 13) & \ mod \ 7
            \end{align*}
            Following these hash functions we can generate a hash table and populate it with the value of hash functions where $i$ is the value of index: \\
            \begin{center}
              \begin{tabular}{c | c | c || c | c | c}
                $i$   & $A$ & $B$ & $(i + 1) \ mod \ 7$ & $2(i + 1) \ mod \ 7$ & $(i + 13) \ mod \ 7$ \\ \hline \hline
                $x_0$ &  1  &  1  &       1       &       2        &      5         \\
                $x_1$ &  1  &  1  &       2       &       3        &      0         \\
                $y_2$ &  1  &  0  &       3       &       6        &      1         \\
                $y_3$ &  1  &  0  &       4       &       1        &      2         \\
                $z_4$ &  1  &  1  &       5       &       3        &      3         \\  \hline
                      &     &     &     $h_1$     &     $h_2$      &     $h_3$      \\
              \end{tabular}
            \end{center}

            The next step would be creating a signature matrix based the hash table:
            \begin{center}
              \begin{tabular}{|c | c c |}\hline
                $i=0$ & $A$ & $B$ \\ \hline
                $h_1$ &  1  &  1  \\
                $h_2$ &  2  &  2  \\
                $h_3$ &  5  &  5  \\  \hline
              \end{tabular}
              \begin{tabular}{|c | c c |}\hline
                $i=1$ & $A$ & $B$ \\ \hline
                $h_1$ &  1  &  1  \\
                $h_2$ &  2  &  2  \\
                $h_3$ &  0  &  0  \\  \hline
              \end{tabular}
              \begin{tabular}{|c | c c |}\hline
                $i=2$ & $A$ & $B$ \\ \hline
                $h_1$ &  1  &  1  \\
                $h_2$ &  2  &  2  \\
                $h_3$ &  0  &  0  \\  \hline
              \end{tabular}
              \begin{tabular}{|c | c c |}\hline
                $i=3$ & $A$ & $B$ \\ \hline
                $h_1$ &  1  &  1  \\
                $h_2$ &  1  &  2  \\
                $h_3$ &  0  &  0  \\  \hline
              \end{tabular}
              \begin{tabular}{|c | c c |}\hline
                $i=4$ & $A$ & $B$ \\ \hline
                $h_1$ &  1  &  1  \\
                $h_2$ &  1  &  2  \\
                $h_3$ &  0  &  0  \\  \hline
              \end{tabular}
            \end{center}
            We are interested in the signature matrix when $i=4$, as that's the last index and it will give us the similarity of $A$ and $B$.
            \begin{align*}
              \begin{tabular}{|c | c c | c | }\hline
                $i=4$ & $A$ & $B$ & similarity\\ \hline
                $h_1$ &  1  &  1  & 1\\
                $h_2$ &  1  &  2  & 0\\
                $h_3$ &  0  &  0  & 1\\  \hline
              \end{tabular}
            \end{align*}
            The $similarity$ column shows us that 2 out 3 hash function found $A$ to be similar to $B$. Which is:
            \begin{align*}
              minHash(A,B) = \dfrac{2}{3} = 0.666\bar{6}
            \end{align*}
            To compare the correctness of our calculations we can have a look at Jaccard coefficient for $A$ and $B$.
            \begin{align*}
              J(A,B) = \dfrac{\| A \cap B \|}{\| A \cup B \|} =
              \frac{ \| \{ x,x,z\} \| }{ \| \{ x,x,y,y,z \} \| } = \frac{3}{5} = 0.6
            \end{align*}

            As you can see $J(A,B) \approx minHash(A,B) \approx 0.6$. By defining more hash functions we can decrease this approximation. Consequently, we proved that the equality holds.
        \end{enumerate}

      \item \textbf{Problem:} $\mathcal{O}(\log{w}\ \log{n})$: \\
        Before analyzing the complexity and responding to the question it could be nice to see an algorithm that finds the $max$ number in an array. Such algorithm can be easily written in python, that gives us a clear picture of how the computation is done:
        \begin{minted}[bgcolor=bg,linenos,fontsize=\small,autogobble]{js}
          def get_max(arr):
            max_num = -1
            for el in arr:
              if max_num < el:
                max_num = el
            return max_num
        \end{minted}

        The best case for finding the $max$ element is when the first element is bigger than all of the other elements of $X$, which has complexity of $\mathcal{O}(1)$. And the worst case is when the last element is the largest and all the consecutive numbers proceed from small to large. This case has a complexity of $\mathcal{O}(n)$. The tricky part is the average case, where we have to compute the complexity for \textit{the number of times that max is assigned to an element}.

        Assuming that $X[1,2,3...m]$ is drawn independently and uniformly at random from the interval $(0,1)$, the expected value will be:
        \begin{align*}
          E[x] = \sum\limits_{i=1}^{m} Pr(X_i)
        \end{align*}
        Where $Pr(X_i)$ is the probability of the \textit{i-th} element being the $max$ element in $X[1,2,3...m]$. To have a better intuition of what could be the probability of \textit{i-th} element we can have a look at different size of $m$:
        \begin{align*}
          Pr(x_1) & = 1 \\
          Pr(x_2) & = \frac{1}{2} \\
          Pr(x_3) & = \frac{1}{3} \\
                  & ... \\
          Pr(x_m) & = \frac{1}{m} \\
        \end{align*}
        Looking at this we can infer that the expected value can be calculated as:
        \begin{align*}
          E[x] = 1 + \frac{1}{2} + \frac{1}{3} + ... + \frac{1}{m} = \sum\limits_{i=1}^{m}{\frac{1}{i}} \approx \log{m}
        \end{align*}
        Consequently, we can conclude that $max = X[i]$ will be executed $\mathcal{O}(\log m)$ times, on expectation.

        \begin{center}
          \includegraphics[width=12cm]{1_sliding_window.png}
          \captionof{figure}{Representation of sliding window: \textit{"CS-E4600 - slide set 7"}.}
        \end{center}

        Using the same logic and intuition we can argue that the \textit{priority sampling for sliding window} will use $\mathcal{O}(\log{w}\log{n})$, where $\log{w}$ is the length of the sliding window and $\log{n}$ is the space for numbers to be stored. The arguments for our prove are the criteria that:
        \begin{itemize}
          \item in any given window each item has equal chance to be selected as a random sample
          \item each removed element has a larger element proceeding it
          \item at any given point we expect the space efficiency $\mathcal{O}(w)$ with maximal elements
          \item and finally, maintaining list of maximal elements requires  $\mathcal{O}(\log{w})$ time
        \end{itemize}
      \item \textbf{Problem:} \textit{Reservoir algorithm for sampling 1 element in a data stream:}
        \begin{enumerate}
          \item \textbf{Question}: \textit{explain k-sample in a data stream is uniform:} \\
            The meaning of $k-sample$ in a data stream being uniform is: for randomly choosing a sample of $k$ items from a data stream, at any given time each element of the data stream have equal probability of being sampled.

          \item \textbf{Question}: \textit{value of the probability p:} \\
            The value of the probability will be the length of $k-sample$ divided by the value of how many elements at current time we have seen, $t$. Which is $\dfrac{k}{t}$.

          \item \textbf{Question}: \textit{prove that value of p gives uniform samples:} \\
            Let's suppose that our $k-sample$ has 5 elements. When sixth element arrives $i=6$ and $t=[1,2...6]$, each element is kept with the probability $\dfrac{5}{6}$, which is:
            \begin{align*}
              (1)(\frac{1}{6} + (\frac{5}{6})(\frac{4}{5})) = \frac{1}{6} + \frac{4}{6} = \frac{5}{6}
            \end{align*}
            when the seventh element arrives $i=7$ and $t=[1,2...7]$, the seventh element is kept with the probability $5/7$ and each of the previous $6$ elements are also kept with the same probability. Following that logic, we can prove by induction that when there are $n$ elements, each one is kept with the probability $5/n$. Consequently, our general notation will be:
            \begin{align*}
              P(k_{sample}|t_{elements\ seen}) = \frac{k}{t}
            \end{align*}

        \end{enumerate}

      \item \textbf{Problem:} \textit{Resort to sampling of good items:}
    \end{enumerate}
\end{document}
